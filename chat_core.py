# ============================================================
# âš« CoreLayer GPT Memory Chat â€” InteraÃ§Ã£o SimbiÃ³tica
# ============================================================
# Usa o Ã­ndice vetorial salvo (Gold) e a memÃ³ria persistente
# para responder perguntas em modo simbiÃ³tico local.
# ============================================================

from pathlib import Path
from ingestion.catalog.catalog_manager import CatalogManager
from core.memory_manager import MemoryManager       # âœ… caminho correto
from core.prompt_layer import PromptLayer
from langchain_ollama import OllamaEmbeddings
from langchain_community.vectorstores import FAISS
from core.rag_pipeline import RagPipeline


def start_chat():
    print("ğŸ§  Iniciando modo simbiÃ³tico de raciocÃ­nio (local)\n")

    # ============================================================
    # ğŸ”¹ Inicializa componentes simbiÃ³ticos
    # ============================================================
    catalog = CatalogManager()
    memory = MemoryManager()
    prompt_layer = PromptLayer()

    # ============================================================
    # ğŸ”¹ Carrega Ã­ndice vetorial existente (FAISS)
    # ============================================================
    index_path = Path("ingestion/gold/knowledge_index")
    if not index_path.exists():
        print("âŒ Nenhum Ã­ndice encontrado. Execute primeiro o pipeline com `python pipeline_run.py`.")
        return

    print("ğŸŸ¡ Carregando Ã­ndice vetorial existente...")
    embeddings = OllamaEmbeddings(model="embeddinggemma")
    index = FAISS.load_local(str(index_path), embeddings, allow_dangerous_deserialization=True)

    # Inicializa pipeline simbiÃ³tico
    rag = RagPipeline(memory, prompt_layer)

    print("\nğŸ§  Pronto para consultas simbiÃ³ticas (digite 'sair' para encerrar)\n")

    # ============================================================
    # ğŸ”¹ Loop principal de interaÃ§Ã£o simbiÃ³tica
    # ============================================================
    thread_id = None
    try:
        while True:
            query = input("â“ Pergunta: ").strip()
            if query.lower() in ["sair", "exit", "quit"]:
                print("\nğŸ§© Encerrando sessÃ£o simbiÃ³tica...")
                memory.close_session()
                break

            # ğŸ”¸ Executa pipeline RAG + memÃ³ria
            response = rag.query_with_memory(query, index)

            # ğŸ”¸ Armazena interaÃ§Ã£o simbiÃ³tica
            thread_id = memory.add(query, response, thread_id=thread_id)

            print(f"\nğŸ’¬ Resposta: {response}\n")

    except KeyboardInterrupt:
        print("\nğŸ§© SessÃ£o interrompida manualmente.")
        memory.close_session()


if __name__ == "__main__":
    start_chat()
