# ============================================================
# âš« core/rag_pipeline.py â€” versÃ£o local (Ollama, compatÃ­vel LC 0.3+)
# ============================================================
from ollama import chat

class RagPipeline:
    def __init__(self, memory, prompt_layer, model="llama3.2"):
        self.memory = memory
        self.prompt_layer = prompt_layer
        self.model = model

    def query_with_memory(self, query, index):
        # ðŸ”¹ Recupera prompt e memÃ³ria
        prompt = self.prompt_layer.get_prompt("default")
        recall = self.memory.recall()

        # ðŸ”¹ Busca contexto nos vetores FAISS
        retriever = index.as_retriever(search_kwargs={"k": 3})

        # âœ… CompatÃ­vel com LangChain >= 0.3
        try:
            docs = retriever.invoke(query)
        except AttributeError:
            # fallback p/ versÃµes antigas
            docs = retriever._get_relevant_documents(query)

        context = "\n".join([d.page_content for d in docs]) + "\n" + recall

        # ðŸ”¹ Monta a mensagem completa
        messages = [
            {
                "role": "system",
                "content": prompt.get(
                    "system",
                    "VocÃª Ã© uma IA simbiÃ³tica local com raciocÃ­nio contextual.",
                ),
            },
            {
                "role": "user",
                "content": f"Pergunta: {query}\n\nContexto:\n{context}",
            },
        ]

        # ðŸ”¹ Chama o modelo local Ollama
        response = chat(model=self.model, messages=messages)

        # ðŸ”¹ Extrai o texto da resposta
        result = response["message"]["content"]

        # ðŸ”¹ Atualiza a memÃ³ria simbiÃ³tica
        self.memory.add(query, result)

        return result
